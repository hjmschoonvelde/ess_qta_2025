---
title: "QTA Lab Session 3: Reading in text data. Inspecting a dfm."
format: 
  #html: default
  gfm: default
editor: source

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = TRUE)
```

In this document we will go through the steps of going from raw texts to a document term matrix that can be analyzed.


## Load libraries

```{r}
#| label: "load_libraries"
#| echo: true
#| message: false
#| warning: false

library(quanteda)
library(stringr)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)

```

## Reading in data

Let's take a look a set of UK prime minister speeches from the [EUSpeech](https://dataverse.harvard.edu/dataverse/euspeech) dataset. 


Read in the speeches as follows using the `read.csv()` function from base `R`: 

```{r}
#| label: "read_in_speeches"
#| echo: true
#| message: false
#| warning: false

speeches <- read.csv(file = "speeches_uk.csv", 
                     header = TRUE, 
                     stringsAsFactors = FALSE, 
                     sep = ",", 
                     encoding = "UTF-8")
```

This `read.csv()` call tells `R` that:

- the file is called "speeches_uk.csv"
- the first row contains the column names
- the columns are separated by commas
- the encoding is UTF-8
- we don't want to turn strings into factors, which is a different data type in `R` that is not useful for text analysis but rather for categorical variables. 

Let's take a look at the structure of this dataset:

```{r}
#| label: "str"
#| echo: true
#| message: false
#| warning: false

str(speeches)

```

As you can see, the corpus contains 787 speeches and variables containing meta data like speaker, country, date, etc. We'll add a variable year that we construct from the date variable. The date variable is currently in the format "dd-mm-yyyy", so we first rely on `as.Date()` to turn this in a date variable. We can then use the `format()` function to extract the year from it.

```{r}
#| label: "year"
#| echo: true
#| message: false
#| warning: false

speeches$year <- format(as.Date(speeches$date, format = "%d-%m-%Y"), "%Y")

```

Take a look at a few speeches. Let's do some very light cleaning on these speeches, using the `stringr` library, in particular the `str_replace_all()` we learned about yesterday. 

```{r}
#| label: "stringr_cleaning"
#| echo: true
#| message: false
#| warning: false

#remove html tags
speeches$text <- str_replace_all(speeches$text, "<.*?>", " ")

#replace multiple white spaces with a single white space
speeches$text <- str_squish(speeches$text)

#we'll lowercase the text, so that we don't have to deal with case sensitivity later on
speeches$text <- tolower(speeches$text)
  
```

Our speeches object is currently a dataframe. To be able to apply functions in `quanteda` on this object it needs to recognize it as a corpus object. To do this we can use the `corpus()` function. We point to the `text` variable in the dataframe that contains the text data using the `text_field` argument. By default the text_field argument assumes that the text data is stored in a variable called "text". If this is not the case, you need to specify the name of the variable that contains the text data.

```{r}
#| label: "create_corpus"
#| echo: true
#| message: false
#| warning: false

corpus_speeches <- corpus(speeches, 
                          text_field = "text")

#the ndoc function displays the number of documents in the corpus
ndoc(corpus_speeches)

```

Metadata such as speaker, date, etc. are stored in a corpus object as docvars, and can be accessed like so (we'll use the `head()` function to limit the output):

```{r}
#| label: "docvars"
#| echo: true
#| message: false
#| warning: false

#date
head(docvars(corpus_speeches, "date"), 10)


#speaker
head(docvars(corpus_speeches, "speaker"), 10)

#number of speeches per speaker

table(docvars(corpus_speeches, "speaker"))

```

Let's tokenize this corpus. We'll use the argument `padding=TRUE` to leave an empty string where the removed tokens previously existed. This is useful if a positional match is needed between the pre- and post-selected tokens, for instance if collocations need to be computed. We'll also remove punctuation, symbols, URLs, and separators. We will not remove numbers, as we may want to keep them for some analyses. We'll also group the tokens by speaker, so that we can later construct a dfm that contains the speeches of each speaker as a separate document.


```{r}
#| label: "tokenize_corpus"
#| echo: true
#| message: false
#| warning: false

tokens_speech <- corpus_speeches %>%
  tokens(what = "word",
         remove_punct = TRUE, 
         padding = TRUE,
         remove_symbols = TRUE, 
         remove_numbers = FALSE,
         remove_url = TRUE,
         remove_separators = TRUE,
         split_hyphens = FALSE) %>%
  tokens_remove(stopwords("en"))
  
```

Let's check the most occurring collocations (this may take a few seconds) that occur 10 times or more. In order to speed things up, we can sample a subset of the tokens object by using the `tokens_sample()` function. We'll use the `min_count` argument to specify that we only want collocations that occur at least 10 times. The lambda statistic is a measure of the strength of the collocation, with higher values indicating stronger collocations.


```{r}
#| label: "collocations"
#| echo: true
#| message: false
#| warning: false


collocations <- tokens_speech %>%
  #tokens_sample(size = 10, replace = FALSE) %>%
  textstat_collocations(min_count = 10) %>%
  arrange(-lambda)

head(collocations, 10)

```

If we want to add the most occurring collocations to the tokens object, we can use the `tokens_compound()` function. Before being able to use this we need to reorganise the collocations object so that it becomes a list of collocations. We can do this by filtering the collocations that occur at least 10 times, pulling the `collocation` column, and then converting it to a phrase object using the `phrase()` function.

```{r}
#| label: "tokens_compound"
#| echo: true
#| message: false
#| warning: false

collocations <- collocations %>%
  filter(lambda > 10) %>%
  pull(collocation) %>%
  phrase()

tokens_speech <- tokens_compound(tokens_speech, collocations)

ndoc(tokens_speech)

```

We'll remove the empty strings that were created by the padding argument in the `tokens()` function. This is done using the `tokens_remove()` function, which removes tokens that match a certain pattern. In this case, we want to remove empty strings.

```{r}
#| label: "remove_empty_strings"
#| echo: true
#| message: false
#| warning: false

tokens_speech <- tokens_remove(tokens_speech, "")

```

Now let's construct a dfm from this tokens object. We'll group this by speaker.

```{r}
#| label: "create_dfm_speaker"
#| echo: true
#| message: false
#| warning: false

speeches_dfm_speaker <- dfm(tokens_speech) %>%
  dfm_group(groups = docvars(., "speaker"))

```

It's straightforward in **quanteda** to inspect a dfm. For example, the `topfeatures()` function displays the most occurring features: 

```{r}
#| label: "topfeatures"
#| echo: true
#| message: false
#| warning: false

topfeatures(speeches_dfm_speaker, 20)

```

You can check the number of features in the dfm using the dim() function: 

```{r}
#| label: "dim"
#| echo: true
#| message: false
#| warning: false

dim(speeches_dfm_speaker)

```
There are over 26,000 features in this dfm. Let's select those tokens that appear at least 10 times by using the `dfm_trim()` function

```{r}
#| label: "min_termfreq"
#| echo: true
#| message: false
#| warning: false

speeches_dfm_speaker = dfm_trim(speeches_dfm_speaker, min_termfreq = 10)
dim(speeches_dfm_speaker)

```
As you can see, this reduces the size of the dfm considerably. However, be mindful that applying such arbitrary cutoffs may remove meaningful features. 

*NB:* Because most words don't occur in most documents, a dfm often contains many zeroes (sparse). Internally, `quanteda` stores the dfm in a sparse format, which means that the zeroes are not stored, so you can create a dfm of many documents and many words without running into memory problems.


## Visualization in **quanteda**

**quanteda** contains some very useful functions to plot your corpus in order get a feel for what is going on. For example, it is easy to construct a wordcloud to see which features appear most often in your corpus.

```{r}
#| label: "textplot_wordcloud"
#| echo: true
#| message: false
#| warning: false

textplot_wordcloud(speeches_dfm_speaker, max_words=50)

```

A slightly more informative frequency plot can be constructed as follows (using the **ggplot2** library):

```{r}
#| label: "ggplot2"
#| echo: true
#| message: false
#| warning: false

speeches_dfm_features <- textstat_frequency(speeches_dfm_speaker, n = 25)

# Sort by reverse frequency order
speeches_dfm_features$feature <- with(speeches_dfm_features, reorder(feature, -frequency))

ggplot(speeches_dfm_features, aes(x = feature, y = frequency)) +
    geom_point() + theme_minimal() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

*NB*  **ggplot2** is a really nice library for making plots and figures. If you have some time after this course is over, I strongly recommend Kieran Healy's [book](https://socviz.co/) on Data Visualization for learning more about effective data viz.


Let's say we are interested in which words are spoken relatively more often by David Cameron than by Tony Blair and Gordon Brown. For this we can use `textstat_keyness()` and `textplot_keyness()` functions. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

head(textstat_keyness(speeches_dfm_speaker, target = "G. Brown"), 10)

textplot_keyness(textstat_keyness(speeches_dfm_speaker, target = "G. Brown"), n = 10)
```

In a next step, let's focus on just the speeches of David Cameron. We can do this by subsetting the dfm using the `dfm_subset()` function. We'll group the dfm by year. 

```{r}
#| label: "cameron_subset"
#| echo: true
#| message: false
#| warning: false


speeches_dfm_year_cameron <- dfm(tokens_speech) %>%
  dfm_subset(speaker == "D. Cameron") %>%
  dfm_group(groups = docvars(., "year"))


dim(speeches_dfm_year_cameron)

```

Let's inspect how often Cameron referred to crisis on a year to year basis. For this group the 'crisis' feature in the dfm by the year variable in the docvars

```{r}
#| label: "crisis_focus"
#| echo: true
#| message: false
#| warning: false

docvars(speeches_dfm_year_cameron, "crisis") <- as.numeric(speeches_dfm_year_cameron[,"crisis"])


```

Let's plot the crisis variable against the year variable


```{r}
#| label: "crisis_plot"
#| echo: true
#| message: false
#| warning: false


df <- docvars(speeches_dfm_year_cameron)

crisis_plot <- ggplot(df, aes(x = year, y = crisis)) +
  geom_point(color = "steelblue", size = 2) +
  labs(title = "Crisis Mentions Over Time",
       x = "Year",
       y = "Frequency of 'crisis'") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12)
  ) +
  ylim(0,100)

print(crisis_plot)

```


## Exercises

Display the most occurring three-word-collocations in tokens_speech

```{r}
#| label: "three_word_collocations"
#| echo: true
#| message: false
#| warning: false


collocations_3 <- tokens_speech %>%
  textstat_collocations(size = 3,
                        min_count = 10) %>%
  arrange(-lambda)

head(collocations_3, 10)

```


Apply `kwic()` to `tokens_speech` object and look up "sinn_fein". Inspect the context in which the Sinn Fein (an Irish nationlist party) is mentioned.

```{r}
#| label: "kwic"
#| echo: true
#| message: false
#| warning: false

kwic(tokens_speech, 
     pattern = phrase("sinn_fein"),
     window = 5)  %>%
  tail()


```


Create a dfm from `tokens_speech` and call it `speeches_dfm`. Group it by speaker. 

```{r}
#| label: "dfm"
#| echo: true
#| message: false
#| warning: false

speeches_dfm <- dfm(tokens_speech) %>%
  dfm_group(groups = docvars(., "speaker"))

```

Check how many documents and features `speeches_dfm` has.

```{r}
#| label: "speeches_check"
#| echo: true
#| message: false
#| warning: false

dim(speeches_dfm) 

```


Trim `speeches_dfm` so that it only contains words that appear in at least 20 times. Inspect the number of features.

```{r}
#| label: "trim"
#| echo: true
#| message: false
#| warning: false

speeches_dfm <- dfm_trim(speeches_dfm, 
                         min_termfreq = 20)

dim(speeches_dfm)

```

Apply `textstat_keyness` to the `speeches_dfm` object to display 10 the most distinctive features for Tony Blair

```{r}
#| label: "tony_blair_plot"
#| echo: true
#| message: false
#| warning: false

head(textstat_keyness(speeches_dfm, target = "T. Blair"), 10)
```


