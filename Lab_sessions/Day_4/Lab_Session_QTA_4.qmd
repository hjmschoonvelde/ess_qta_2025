---
title: "QTA lab Session 4: Categorizing text using dictionaries"
format: 
  #html: default
  gfm: default
editor: source

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = TRUE)
```

This document describes how to use dictionary methods in **quanteda**. Let's first load the required libraries. If you haven't installed **quanteda.sentiment** yet, you can do so by running `remotes::install_github("quanteda/quanteda.sentiment")`, since this package is not yet on CRAN, so you need to install it from the GitHub page of the developer. 

```{r}
#| label: "load_libraries"
#| echo: true
#| message: false
#| warning: false

library(quanteda)
library(stringr)
library(tidyverse)
library(quanteda.sentiment)

```

Let's save the inaugural speeches as an object `speeches_inaugural` just like we did in the previous lab session.

```{r}
#| label: "speeches_inaugural"
#| echo: true
#| message: false
#| warning: false

speeches_inaugural <- data_corpus_inaugural

```

We'll first tokenize this corpus.

```{r}
#| label: "dfm_inaugural"
#| echo: true
#| message: false
#| warning: false

tokens_inuagural <- tokens(speeches_inaugural,
                           what = "word",
                           remove_punct = TRUE, 
                           remove_symbols = TRUE, 
                           remove_numbers = FALSE,
                           remove_url = TRUE,
                           remove_separators = TRUE,
                           split_hyphens = FALSE,
                           padding = FALSE
                       )

```

In a next step, we'll include negations of positive and negative sentiment words. This is done by using the `tokens_compound` function, which allows you to create compound tokens based on a dictionary of negations. We'll also create a dfm called `dfm_inaugural` that contains the tokenized inaugural speeches. 

```{r}
#| label: "tokens_compound"
#| echo: true
#| message: false
#| warning: false


tokens_inuagural <- tokens_compound(tokens_inuagural, pattern = phrase("not *"))

dfm_inaugural <- dfm(tokens_inuagural)

```

## Off-the shelf dictionaries ##

**quanteda.sentiment** contains a number of off-the-shelf sentiment dictionaries that allow you to assess the sentiment of a text. These dictionaries are stored as dictionary objects. Let's take a look at the Lexicoder Sentiment Dictionary (LSD) from Young and Soroka (2012)  and the NRC Word-Emotion Association Lexicon from Mohammad and Turney (2013). They are stored in `quanteda.sentiment` as dictionary object under `data_dictionary_LSD2015` and `data_dictionary_NRC` respectively. 

```{r}
#| label: "LSD"
#| echo: true
#| message: false
#| warning: false

summary(data_dictionary_LSD2015)

print(data_dictionary_LSD2015, max_nval = 5)


```

We can use `dfm_lookup` to apply it to our dfm containing inaugural speeches. This results in a dfm with the same number of rows as the original dfm, but with the columns containing the number of positive and negative sentiment words (and their negations)

```{r}
#| label: "dfm_lookup"
#| echo: true
#| message: false
#| warning: false

dfm_inaugural_LSD <- dfm_lookup(dfm_inaugural, 
                                dictionary = data_dictionary_LSD2015)

head(dfm_inaugural_LSD)
```



We can calculate the relative fraction of negative sentiment tokens to positive sentiment tokens in each speech by dividing the number of sentiment tokens by the total number of tokens in the speech. We can save this as a variable in the docvars of the dfm object. 

```{r}
#| label: "sentiment_scores"
#| echo: true
#| message: false
#| warning: false

#number of negative words
docvars(dfm_inaugural, "neg_words") <- as.numeric(dfm_inaugural_LSD[,1]) + as.numeric(dfm_inaugural_LSD[,3])

#number of positive words
docvars(dfm_inaugural, "pos_words") <- as.numeric(dfm_inaugural_LSD[,2]) + as.numeric(dfm_inaugural_LSD[,4])

#sentiment score
docvars(dfm_inaugural, "LSD_sentiment")  <-  100*(docvars(dfm_inaugural, "pos_words") - docvars(dfm_inaugural, "neg_words"))/ntoken(dfm_inaugural)

docvars(dfm_inaugural, c("President", "LSD_sentiment"))
```

Let's do the same, but this time with the NRC Word-Emotion Association Lexicon

```{r}
#| label: "NRC"
#| echo: true
#| message: false
#| warning: false

dfm_inaugural_NRC <- dfm_lookup(dfm_inaugural, 
                                dictionary = data_dictionary_NRC)

#number of negative words (NB: these are stored in the 6th column in the dfm)
docvars(dfm_inaugural, "neg_NRC_words") <- as.numeric(dfm_inaugural_NRC[,6])

#number of positive words (NB: these are stored in the 7th column in the dfm)
docvars(dfm_inaugural, "pos_NRC_words") <- as.numeric(dfm_inaugural_NRC[,7])

#sentiment score
docvars(dfm_inaugural, "NRC_sentiment")  <- 100*(docvars(dfm_inaugural, "pos_NRC_words") - docvars(dfm_inaugural, "neg_NRC_words"))/ntoken(dfm_inaugural)

head(docvars(dfm_inaugural, c("President", "NRC_sentiment")))
```

Let's plot the correlation between the two sentiment measures. 

```{r}
#| label: "correlation_sentiment_dictionaries"
#| echo: true
#| message: false
#| warning: false

cor(docvars(dfm_inaugural, "LSD_sentiment"), docvars(dfm_inaugural, "NRC_sentiment"))

correlation_plot_LSD_NRC <- ggplot(docvars(dfm_inaugural), aes(LSD_sentiment, NRC_sentiment)) + 
  geom_point(pch = 21, fill = "gray25", color = "white", size = 2.5) +
  scale_x_continuous(name = "NRC sentiment") +
  scale_y_continuous(name = "LSD sentiment") +
  theme_minimal()

print(correlation_plot_LSD_NRC)

```

The correlation of 0.76 is reassuring since both measures should be capturing the same construct 

As a last step we'll inspect if Presidents make use of narrative arches in their speeches. For example, they may start a speech more subdued and end on a more positive note. Or they may start positive and end positive. Let's first create a paragraph-based dfm of Obama's inaugural speeches

```{r}
#| label: "narrative_arches"
#| echo: true
#| message: false
#| warning: false

obama_corpus <- corpus_subset(speeches_inaugural, President == "Obama") %>%
  corpus_reshape(to =  "paragraph")

ndoc(obama_corpus)

obama_tokens <- tokens(obama_corpus,
                           what = "word",
                           remove_punct = TRUE, 
                           remove_symbols = TRUE, 
                           remove_numbers = FALSE,
                           remove_url = TRUE,
                           remove_separators = TRUE,
                           split_hyphens = FALSE,
                           padding = FALSE
                       )

obama_dfm <- dfm(obama_tokens)

```

Let's apply the NRC dictionary to this dfm object. 

```{r}
#| label: "narrative_arch_NRC"
#| echo: true
#| message: false
#| warning: false

obama_dfm_NRC <- dfm_lookup(obama_dfm, 
                                dictionary = data_dictionary_NRC)

docvars(obama_dfm, "neg_words") <- as.numeric(obama_dfm_NRC[,6])

docvars(obama_dfm, "pos_words") <- as.numeric(obama_dfm_NRC[,7])

#sentiment score
docvars(obama_dfm, "NRC_sentiment")  <- 100*(docvars(obama_dfm, "pos_words") - docvars(obama_dfm, "neg_words"))/ ntoken(obama_dfm)
```

Let's plot this information as a sequence of paragraphs over time. We can use the `docvars` function to extract the year of the speech and use it as a facet variable. 

```{r}
#| label: "time"
#| echo: true
#| message: false
#| warning: false

table(docvars(obama_dfm, "Year"))

#the following code will create a variable sentence in the docvars of obama_dfm that contains a paragraph counter

docvars(obama_dfm, "sentence") <- NA
docvars(obama_dfm, "sentence")[1:36] <- 1:36
docvars(obama_dfm, "sentence")[37:65] <- 1:29

obama_plot <- ggplot(docvars(obama_dfm), aes(sentence, NRC_sentiment)) + 
  geom_smooth() +
  scale_x_continuous(name = "Sentence") +
  scale_y_continuous(name = "NRC sentiment") +
  theme_minimal() + facet_grid(~Year)

print(obama_plot)
```

## Self made dictionaries

When working with your own dictionary, most of the work will go into evaluating its validity and reliability in order to make sure that it captures the construct that you are looking for. However, once you have settled on a dictionary, it is easy in **quanteda** to apply it to a corpus. 

Let's say where are interested in how often these presidents refer to the economy. Let's create a dictionary object `econ_dict` that contains words that are related to the economy. A dictionary object is a list object with the key being the category and the values being the words that belong to that category, represented as a character vector. The asterisk is a wildcard that allows you to match words that start with the same letters.

```{r}
#| label: "self_made_dictionaries"
#| echo: true
#| message: false
#| warning: false

#create a dictionary
econ_dict <- dictionary(list(
  Economy = c(
    "econ*", "job*", "employ*", "unemploy*", "labor*", "labour*", 
    "income*", "wage*", "pay*", "salary*", "work*", "worker*", 
    "industr*", "business*", "firm*", "market*", "trade*", "export*", 
    "import*", "gdp", "growth*", "recession*", "inflation*", 
    "deflation*", "investment*", "productiv*", "profit*", 
    "consumption*", "demand*", "supply*", "crisis*", "financ*", 
    "credit*", "debt*", "bank*", "tax*", "budget*", "subsid*", 
    "bailout*", "regulation*", "monetary*", "fiscal*", 
    "cost*", "price*", "econom*", "development*", "capital*", 
    "entrepreneur*", "small business*", "stock*", "share*", 
    "exchange*", "interest rate*", "central bank*", "pension*", "saving*"
  )
))

econ_dict_dfm <- dfm_lookup(dfm_inaugural, 
                            dictionary = econ_dict)

dim(econ_dict_dfm)
tail(econ_dict_dfm)
```


If we want to average the average number of mentions per speaker we can save these dictionary results as a variable in our corpus object. Let's call it `economy`.

```{r}
#| label: "economy_variable"
#| echo: true
#| message: false
#| warning: false

docvars(speeches_inaugural, "economy") <- 100*as.numeric(econ_dict_dfm) / ntoken(dfm_inaugural)

```

Let's plot the fraction of economy words over time. We'll add a smooth line to the plot to make the trend more visible.

```{r}
#| label: "economy_over_time"
#| echo: true
#| message: false
#| warning: false

lineplot_economy <- ggplot(docvars(speeches_inaugural),
                                aes(x = Year, y = economy)) +
  geom_smooth(method = "loess",
              span = 0.2,
              color = "#56B4E9") + 
  theme_minimal()


print(lineplot_economy)

```

## Excercise

Create a dictionary titled `threat_dictionary`, with threat as a key and threat* peril* risk* danger* hazard* menace* attack* violence* crisis* instabil* emergency* conflict* terror* hostil* aggress* assault* disaster* catastroph* security* insecurity* as values


```{r}
#| label: "threat_dictionary"
#| echo: true
#| message: false
#| warning: false
#your answer here
```

Apply this dictionary to `dfm_inaugural` and call the resulting object `dfm_inaugural_threat`. Append the results in the docvars of `speeches_inaugural` as a variable `threat` containing the fraction of threat words in each speech

```{r}
#| label: "dfm_inaugural_threat"
#| echo: true
#| message: false
#| warning: false
#your answer here
```

Plot the fraction of threat words over time


```{r}
#| label: "threat_over_time"
#| echo: true
#| message: false
#| warning: false

#your answer here
```

Apply the NRC emotion detection lexicon to `dfm_inaugural` and append a varioble called nrc_fear as metadata to `speeches_inaugural` that contains the fraction of NRC fear words in each speech.

```{r}
#| label: "fear_dictionary"
#| echo: true
#| message: false
#| warning: false
#your answer here
```

Plot the fraction of fear words over time

```{r}
#| label: "fear_over_time"
#| echo: true
#| message: false
#| warning: false

#your answer here

```

Calculate the correlation between nrc_fear and threat, and produce a scatterplot

```{r}
#| label: "correlation"
#| echo: true
#| message: false
#| warning: false

#your answer here

```

Reflect on these results

```{r}
#| label: "reflection"
#| echo: true
#| message: false
#| warning: false

#your answer here
```
